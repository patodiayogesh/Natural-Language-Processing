{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "# Imports - our files\n",
    "import utils\n",
    "import models\n",
    "#import argparse\n",
    "\n",
    "# Global definitions - data\n",
    "DATA_FN = '/Users/neelampatodia/Desktop/Yogesh/NLP/Assignments/hw2/data/crowdflower_data.csv'\n",
    "LABEL_NAMES = [\"happiness\", \"worry\", \"neutral\", \"sadness\"]\n",
    "\n",
    "# Global definitions - architecture\n",
    "EMBEDDING_DIM = 100  # We will use pretrained 100-dimensional GloVe\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "USE_CUDA = torch.cuda.is_available()  # CUDA will be available if you are using the GPU image for this homework\n",
    "\n",
    "# Global definitions - saving and loading data\n",
    "FRESH_START = False  # set this to false after running once with True to just load your preprocessed data from file\n",
    "#                     (good for debugging)\n",
    "TEMP_FILE = \"temporary_data.pkl\"  # if you set FRESH_START to false, the program will look here for your data, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataLoaders and embeddings from file....\n"
     ]
    }
   ],
   "source": [
    "with open(TEMP_FILE, \"rb\") as f:\n",
    "    print(\"Loading DataLoaders and embeddings from file....\")\n",
    "    train_generator, dev_generator, test_generator, embeddings, train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetwork(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, layer_dim, output_dim,embeddings):\n",
    "        super(RecurrentNetwork, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(embeddings.shape[0],\n",
    "                                            embeddings.shape[1])\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=layer_dim, batch_first=True)#, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    # x is a PaddedSequence for an RNN\n",
    "    def forward(self, x):\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Fill in the forward pass of your neural network.\n",
    "        # TODO: (The backward pass will be performed by PyTorch magic for you!)\n",
    "        # TODO: Your architecture should...\n",
    "        # TODO: 1) Put the words through an Embedding layer (which was initialized with the pretrained embeddings);\n",
    "        # TODO: 2) Feed the sequence of embeddings through a 2-layer RNN; and\n",
    "        # TODO: 3) Feed the last output state into a dense layer to become a 4-vector of values, one for each class\n",
    "        output = self.embedding_layer(x)\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        out, h1 = self.rnn(output, h0)\n",
    "        #out, h2 = self.rnn(out,h1)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe9d7e15df0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecurrentNetwork(100, 110, 2, 4,embeddings)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [01:01<29:31, 61.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0 tensor([29.7429]) tensor([70.2571])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [02:20<33:28, 71.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1 tensor([27.6676]) tensor([2.0753])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [03:43<34:37, 76.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2 tensor([26.5271]) tensor([1.1406])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [05:12<35:20, 81.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3 tensor([26.2748]) tensor([0.2523])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [06:38<43:11, 99.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 4 tensor([26.7415]) tensor([0.4667])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "development_loss = 100.0\n",
    "continous_negative = 0\n",
    "for n in tqdm.tqdm(range(30)):\n",
    "    avg_loss = []\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for x, y in train_generator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, y)\n",
    "        avg_loss.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*x.size(0)\n",
    "    \n",
    "    #print('train loss', train_loss)\n",
    "    gold = []\n",
    "    predicted = []\n",
    "    # Keep track of the loss\n",
    "    loss = torch.zeros(1)  # requires_grad = False by default; float32 by default\n",
    "    if USE_CUDA:\n",
    "        loss = loss.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in dev_generator:\n",
    "            y_pred = model(X_b)\n",
    "            # Save gold and predicted labels for F1 score - take the argmax to convert to class labels\n",
    "            gold.extend(y_b.cpu().detach().numpy())\n",
    "            predicted.extend(y_pred.argmax(1).cpu().detach().numpy())\n",
    "            loss += loss_fn(y_pred.double(), y_b.long()).data\n",
    "    print('loss',n, loss, abs(development_loss-loss))\n",
    "    if development_loss-loss<0:\n",
    "        break\n",
    "    development_loss = loss\n",
    "    m=model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loss_fn, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model on the development set, providing the loss and macro F1 score.\n",
    "    :param model: a model that performs 4-way emotion classification\n",
    "    :param loss_fn: a function that can calculate loss between the predicted and gold labels\n",
    "    :param test_generator: a DataLoader that provides batches of the testing set\n",
    "    \"\"\"\n",
    "    gold = []\n",
    "    predicted = []\n",
    "\n",
    "    # Keep track of the loss\n",
    "    loss = torch.zeros(1)  # requires_grad = False by default; float32 by default\n",
    "    if USE_CUDA:\n",
    "        loss = loss.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over batches in the test dataset\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in test_generator:\n",
    "            # Predict\n",
    "            y_pred = model(X_b)\n",
    "\n",
    "            # Save gold and predicted labels for F1 score - take the argmax to convert to class labels\n",
    "            gold.extend(y_b.cpu().detach().numpy())\n",
    "            predicted.extend(y_pred.argmax(1).cpu().detach().numpy())\n",
    "\n",
    "            loss += loss_fn(y_pred.double(), y_b.long()).data\n",
    "\n",
    "    # Print total loss and macro F1 score\n",
    "    print(\"Test loss: \")\n",
    "    print(loss)\n",
    "    print(\"F-score: \")\n",
    "    print(f1_score(gold, predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: \n",
      "tensor([27.3810])\n",
      "F-score: \n",
      "0.4143990964422248\n"
     ]
    }
   ],
   "source": [
    " test_model(m, loss_fn, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
