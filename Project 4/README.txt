Yogesh Patodia
yp2607@columbia.edu

Problem Statement:
Use of Image Data along with their captions to generate captions of images.

Data: Flickr Dataset
Please submit download request for data here: https://forms.illinois.edu/sec/1713398

We train a conditional generation model for image captioning.

We use  inception_v3 model to encode the images.
After Data Preparation we train the model on the images along with their caption sequences.

Language Generation Model:
    We use a single layer Bi-directional LSTM model for encoding
    We will project the 2048-dimensional image encoding to a 300-dimensional hidden layer.
    We then concatenate this vector with each embedded input word, before applying the LSTM.

Decoder:
We implement the following Decoders:
    Greedy Decoder
    Beam Search Decoder (beam size=3,5)
    Nucleus Sampling Decoder (Probability mass = 0.95)

We implement the BLEU metric to evaluate the captions generated by each decoder.
